# Architext example .env file
# Copy this file to `.env` and fill in any secrets or local overrides.
# Do NOT commit your `.env` file with secrets to source control.

# LLM configuration
LLM_PROVIDER=local                     # 'local' or 'openai'
LLM_MODEL=local-model                  # model identifier used by local LLM or provider
OPENAI_API_BASE=http://127.0.0.1:5000/v1
OPENAI_API_KEY=sk-                         # fill for OpenAI or compatible providers
LLM_TEMPERATURE=0.1
LLM_MAX_TOKENS=512
SYSTEM_PROMPT=

# Embeddings
EMBEDDING_PROVIDER=huggingface          # 'huggingface' or 'openai'
EMBEDDING_MODEL_NAME=sentence-transformers/all-mpnet-base-v2
EMBEDDING_CACHE_DIR=models_cache

# Retrieval / chunking
CHUNK_SIZE=512
TOP_K=5
CHUNKING_STRATEGY=logical               # 'logical' or 'file'

# Hybrid / rerank (advanced)
ENABLE_RERANK=false
RERANK_MODEL=cross-encoder/ms-marco-MiniLM-L-6-v2
RERANK_TOP_N=10
ENABLE_HYBRID=false
HYBRID_ALPHA=0.7

# Storage / vector DB
STORAGE_PATH=./storage
VECTOR_STORE_PROVIDER=chroma           # options: chroma, qdrant, pinecone, weaviate
VECTOR_STORE_COLLECTION=architext_db

# Optional provider credentials
QDRANT_URL=
QDRANT_API_KEY=
PINECONE_API_KEY=
PINECONE_INDEX_NAME=
WEAVIATE_URL=
WEAVIATE_API_KEY=

# Server / security
RATE_LIMIT_PER_MINUTE=120
ALLOWED_SOURCE_ROOTS=
ALLOWED_STORAGE_ROOTS=
TASK_STORE_PATH=~/.architext/task_store.json

# Notes:
# - For local development you can set LLM_PROVIDER=local and run a local LLM server (e.g., ollama/oobabooga).
# - Use OPENAI_API_BASE and OPENAI_API_KEY for OpenAI-compatible endpoints.
# - Run `python -m src.cli <command> --help` for command-specific flags.
